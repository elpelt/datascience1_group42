
The K-Medoids clustering method is related to the above described K-Means algorithm, but uses medoids (representative points for each cluster) instead of means to define new cluster centers, which makes it more robust to outliers\cite{Jin2010}. It partitions the dataset into $k$ non-overlapping clusters by assigning each of the $n$ data points to the closest of $k$ cluster centers, which are defined by the most centrally located medoids. A medoid is a point with a minimal average dissimilarity (which is computed based on the chosen distance measure) to all other data points in the same cluster. The most commonly used algorithm to solve this NP-hard problem heuristically is the PAM (Partitoning Around Medoids) algorithm, which works in a greedy way: \cite{kaufman2009finding} \\
\begin{enumerate}
	\item First greedily initialize the algorithm by selecting $k$ data points to be the medoids and assigning every data point to its closest medoid. The initial data points can also be found by a K++ approach (similarly used by K-Means, see section \ref{kplusplusdef}) \cite{scikit-learn-extra}. Note that for a possible non-deterministic initialization with random initial points the obtained clusters may differ for different runs of the algorithm. \\
	\item Compare the average dissimilarity coefficient of a swap of each medoid $m$ and a non-medoid data point $\bar{m}$. Find a swap between $m$ and $\bar{m}$ that would decrease the average dissimilarity coefficient the most. 
	\item If no change of a medoid happened in the second step, terminate the algorithm, else reassign the data points to the new medoids and go back to step 2. 
\end{enumerate}

The runtime complexity of the algorithm is $O(n^2kt)$ where $t$ is the number of iterations until convergence  \cite{scikit-learn-extra}. A disadvantage of K-Medoids is that the number of clusters has to be predefined (which is the case for all K-Algorithms). 