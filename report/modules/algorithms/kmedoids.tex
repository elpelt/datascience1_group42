\marginnote{\textcolor{blue}{Jonas Elpelt}}

The K-Medoids clustering method is related to the well-known K-means algorithm, but uses medoids (representative points for each cluster) instead of means to define new cluster centers, which makes it more robust to outliers\cite{Jin2010}. It partitions the dataset into $k$ non-overlapping clusters by assigning each data point to the closest of $k$ cluster centers, which are defined by the most centrally located medoids. A medoid is a point with a minimal average dissimilarity (which is computed based on the chosen distance measure) to all other data points in the same cluster. The most commonly used algorithm to solve this NP-hard problem heuristically is the PAM (Partitoning Around Medoids) algorithm, that works in a greedy way: \cite{kaufman2009finding} \\
\begin{enumerate}
	\item First initialize the algorithm by selecting $k$ data points to be the medoids and assigning every data point to its closest medoid. The initial data points can be found by a k++ approach (similarly used by kmeans) \cite{scikit-learn-extra}. (Note that for a non-deterministic initialization with random initial points the obtained clusters may differ for different runs of the algorithm.) \\
	\item Compare the average dissimilarity coefficient of a swap of each medoid $m$ and a non-medoid data point $\bar{m}$. Find a swap between $m$ and $\bar{m}$ that would decrease the average dissimilarity coefficient the most. 
	\item If no change of a medoid happened in the second step, terminate the algorithm, else re-assign the data points to the new medoids and go back to step 2 (until convergence). 
\end{enumerate}
