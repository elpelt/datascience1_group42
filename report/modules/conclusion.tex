


For Wine and Housevotes datasets we found that a clustering with the Chebyshev distance produces clusterings with consideribly low scores for all evaluated indices. 
For Iris and Wine datasets ARI scores for the used \Gls{glos:K-Algorithms} have a peak for k=3 (\autoref{fig:comparison_iris}, \autoref{fig:comparison_wine}), which corresponds to the true number of labels in these datasets.  
By comparing different clustering algorithms for the Wine dataset \autoref{fig:comparison_wine} we found that the K-median algorithm is more unstable for this specific dataset than K-means and K-medoids as small peturbations of k can lead to remarkable differences in the resulting clustering scores. 

In our test cases the Cosine distance always took the longest computation time for all clustering algorithms and datasets. The Euclidean distance had longer runtimes than Manhattan distance and Chebyshev distance. 

For the Iris dataset a clustering calculated by a K-algorithm (with k=3, \autoref{fig:comparison_iris}) with the Manhattan distance gives the highest score values for all evaluated external scores. Therefore, the resulting clustering labels calculated with the Manhattan distance evidentely match the true labels in the most accurate way in comparison with all other considered distance measures. Comparing the performances of all \Gls{glos:K-Algorithms} we found that K-medians and K-medoids give slightly higher scores than K-means. The results are very similar and differ only in a few differently assigned data points. \\
Using the Chebyshev distance for the Wine dataset results in clusterings with the lowest scores compared to all other distance measures. In contrast, k-means with the Euclidean distance and K-medians with the Manhattan distance perform relatively well for this dataset (\autoref{fig:comparison_wine}). However, the Chebyshev distance is not unsuitable for other datasets like the Iris dataset. Here, the distance performes similarly well as the other distance measures.
For the Diabetes dataset the K-medians algorithm with k=2 and the Manhattan distance performs the worst considering the Silhouette Score (\autoref{fig:comparison_diabetes}). The K-means algorithm along with the Euclidean, Manhattan or Cosine distance, as well as the results of the K-medians or K-medoids algorithm with Cosine distance, give the highest scoring results. \\
For the Housevotes dataset (\autoref{fig:comparison_housevotes}) the Chebyshev distance performs the worst. The highest external scores for k=2 can be achieved with K-Means using the Euclidean or Manhattan distance. \\


In general, our study is restricted to a small set of algorithms and distance measures. As a result, it can only provide a limited overview. A variety of additional algorithms, distance metrics, parameter settings and cluster evaluation measures could be considered to get a more exhaustive in-depth analysis. By including more datasets a more detailed comparison of the algorithms and distance measures and their behaviour on differently distributed data might be possible. Furthermore, even more clustering indices, particularly internal indices, could give further insights on the clustering quality. \\

Finally, we did not find the best clustering algorithm or the superior distance measure which would give the best results on every dataset. This general problem in the field of Data Science is widely known as the "No Free Lunch Theorem" \cite{nofreelunch}. As specific requierements may differ for diverse use cases multiple algorithms and parameters should be explored and compared according to individual needs. 
