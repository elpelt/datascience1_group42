% !TeX spellcheck = <none>
\documentclass[12pt, english]
{article}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{microtype}
\usepackage[automake, nonumberlist, acronym, toc, section]{glossaries}
\usepackage{cite}
\usepackage[parfill]{parskip}
\usepackage{amssymb}
\usepackage{textcmds}
\usepackage{hyperref}
\usepackage{float}
\usepackage[toc,page]{appendix}
\usepackage{marginnote}
\usepackage{xcolor}
\usepackage{booktabs}

\newglossary[slg]{symbolslist}{syi}{syg}{Symbols}

\makeglossaries

%commands for symbols
\newglossaryentry{symb:Pi}{
name=$\pi$,
description={You know it.},
sort=symbolpi, type=symbolslist
}
\newglossaryentry{symb:Phi}{
name=$\varphi$,
description={At vero eos et accusam et justo duo dolores et ea rebum..},
sort=symbolphi, type=symbolslist
}
\newglossaryentry{symb:Lambda}{
name=$\lambda$,
description={Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.},
sort=symbollambda, type=symbolslist
}
 
%commands for abbreviations

% ...

%abbreviations and glossary combined
\newacronym{AD}{AD}{Active Directory\protect\glsadd{glos:AD}}
\newacronym{t-SNE}{t-SNE}{t-Distributed Stochastic Neighbor Embedding}
\newacronym{pca}{PCA}{Principal Component Analysis}
\newacronym{ari}{ARI}{Adjusted Rand Index}
\newacronym{ami}{AMI}{Adjusted Mutual Info Score}
\newacronym{sc}{SC}{Silhouette coefficient}
\newacronym{mi}{MI}{Mutual Information}
\newacronym{dbscan}{dbscan}{Density-Based Spatial Clustering of Applications with Noise}
\newacronym{minPts}{MinPts}{Minimum number of points to be considered in Eps-neighourhood for DBSCAN}

 
% ... 
 
%glossary commands

\newglossaryentry{glos:K-Algorithms}{
name=K-Algorithms,
description={One of the studied clustering algorithms, that work with a parameter k: K-Means, K-Medians, K-Medoids}
}

\newglossaryentry{glos:onehotencoding}{
	name=One-Hot-Encoding,
	description={Encoding method for categorical attribute values where every attribute is represented as a binary vector and each element in the vector represents a category.}
}

\newglossaryentry{glos:rangequery}{
	name=RangeQuery,
	description={Function to find all points in a Eps-Neighbourhood around a given point.}
}
% ...

\patchcmd{\subequations}{\def\theequation{\theparentequation\alph{equation}}}%
{\def\theequation{\theparentequation.\arabic{equation}}}{}{}

\begin{document}

\begin{titlepage}

\begin{center}

{\Huge {
Term Paper Data Science 1}
}
\\[2ex]

\textbf{
\Large 
Docent: Prof. Dr. Lena Wiese \\ 
Semester: Summer Term 2021\\  
}



\includegraphics[scale=0.4]{logo.jpg} \\ 
\large{\textbf{Institute of Computer Science \\ Goethe-Universit\"at Frankfurt a. M.}}



\begin{tabular}{ll}
Authors: & \textsc{Franziska Hicking} \\
& {\small 6673525} \\
& {\small franziska.hicking@stud.uni-frankfurt.de} \\
& {\small Master Bioinformatics, 2} \\
& \textsc{Jonas Elpelt} \\
& {\small 6673181}\\
&{\small  elpelt@stud.uni-frankfurt.de}\\
& {\small Master Bioinformatics, 2} \\
& \textsc{Julian Rummel} \\
&{\small  6673334}\\
& {\small s9594673@stud.uni-frankfurt.de}\\
&{\small  Master Bioinformatics, 2} \\
& \textsc{Niklas Conen}\\
& {\small 6599913}\\
& {\small conen@stud.uni-frankfurt.de}\\
& {\small Bachelor Computer Science, 8}\\
Date: & \today \\		
\end{tabular}

\end{center}

\vspace*{\fill}

\large
\noindent{}Chosen Project Topic: \\
T4 - DISTANCE MEASURES AND CLUSTERING \\
\small Github Repository: \url{https://github.com/elpelt/datascience1\_group42/}




\end{titlepage}

\newpage%\thispagestyle{empty}~ %empty page
All authors confirm that they contributed equally to the project report and were involved in the implementation and evaluation to the same extent.
\newpage 

\begin{abstract}
Clustering algorithms can be important tools during the analysis of datasets. They divide a dataset into groups of items based on a certain measure of similarity such as the distances between each of the items. 
In this work, we implemented and compared four different clustering algorithms (K-Means, K-Medoids, K-Medians, DBSCAN). For this, we selected four distinct datasets as well as multiple distance measures (Manhattan, Euclidean, Angular cosine, Chebyshev). For efficient comparison of the clustering results we made use of multiple clustering indices. Additionally, we implemented a web frontend which provides the ability to run all clustering algorithms with distance measures, datasets and clustering indices chosen by the user. The results will be visualized afterwards.
After running all algorithms with each of the datasets respectively and all distance measures where they could be applied, we compared the resulting values of the clustering indices.
Finally we tried to estimate the best parameter combinations for each dataset.

\end{abstract}

\newpage

\tableofcontents

\newpage


\section{Definition of Distance Measure}
\label{def_DM}
\input{modules/distances/definition}
\section{Different Distance Measurements} \label{distances}
\subsection{Manhattan Distance}
\input{modules/distances/manhattan}
\subsection{Euclidean Distances}
\input{modules/distances/euclidean}

\subsection{Angular Cosine Distance}
\input{modules/distances/cosine}

\subsection{Chebyshev Distance}
\input{modules/distances/chebyshev}

\section{Data Set Description}

\subsection{Housevotes} \label{housevotes}
\input{modules/datasets/housevotes}
\subsection{Wine}
\input{modules/datasets/wine}
\subsection{Iris}
\input{modules/datasets/iris}
\subsection{Diabetes} \label{diabetes}
\input{modules/datasets/diabetes}

\section{Clustering Algorithms} \label{algorithms}
\subsection{K-Means}
\input{modules/algorithms/kmeans}
\subsection{K-Medoids}
\input{modules/algorithms/kmedoids}
\subsection{K-Medians}
\input{modules/algorithms/kmedians}
\subsection{DBSCAN}
\input{modules/algorithms/dbscan}

\section{Additional Methods Used}
\subsection{k++-Initializer}
\input{modules/kplusplus}
\subsection{t-SNE}
\input{modules/dim_reduction_methods/tsne}
\subsection{Principal Component Analysis (PCA)}
\input{modules/dim_reduction_methods/pca}
\subsection{DBSCAN Parameter Estimation Heuristic} \label{dbscanheuristic}
\input{modules/additional/dbscanheuristic}

\section{Implementation}
\subsection{Description}
\input{modules/impl/desc}
\subsection{Dependencies}
\input{modules/impl/libraries}

\section{Evaluation Module}
\subsection{Implementation}
\input{modules/Indices/Implementation_ind}
\subsection{External Scoring Methods}
\subsubsection{Definition}
\input{modules/Indices/external_def}
\subsubsection{Adjusted Rand Index}
\input{modules/Indices/ARI}
\subsubsection{Completeness Score}
\input{modules/Indices/Comp_score}
\subsubsection{Homogeneity Score}
\input{modules/Indices/Hom_score}
\subsubsection{Adjusted Mutual Index}
\input{modules/Indices/NMI}
\subsubsection{Justification}
\input{modules/Indices/justification}
\subsection{Internal Scoring Methods}
\subsubsection{Definition}
\input{modules/Indices/internal_def}
\subsubsection{Silhouette Score}
\input{modules/Indices/Silhouette_Score}


\section{Web Frontend and User Manual}
\input{modules/web_frontend/user_manual}

\section{Conclusion}
\input{modules/conclusion}

\newpage

%print glossary
\printglossary[style=altlist,title=Glossary]
 
%print abbreviations
\printglossary[type=\acronymtype,style=long]
 
%print symbols
%\printglossary[type=symbolslist,style=long]


\newpage


\bibliography{bib.bib}
\bibliographystyle{unsrt}
\nocite{*}

\begin{appendices}
	\input{modules/appendix}
\end{appendices}

\end{document}
